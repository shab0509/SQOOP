SQOOP = SQL + OOP

Sqoop is the most prominent Data Ingestion Tool for moving the data from RDBMS to HDFS.
Moreover, to import and export the data, Sqoop uses MapReduce. Also, offers parallel operation as well as fault tolerance.

Basically, Sqoop attains the collection of related tools. 
We can use Sqoop, by just specifying the tool we want to use but with the arguments that control the tool.

INSTALL MYSQL: sudo apt-get instal mysql-server mysql-client

Basically sqoop is the component of hadoop built on top of HDFS & exclusively meant for communicating with RDBMS.
  IMPORT : Move data RDBMS to HDFS
  EXPORT : Move data HDFS to RDBMS

However Sqoop is a data ingestion tool, however it is not meant for processing of the data with some business logic like MR,pig ....

Some key features of hadoop:

* Either sqoop import/export  will happen with respect to HDFS only.
* Sqoop will communicate with HDFS only.
* if we are communicating with some RDBMS from hadoop using sqoop, 
  (also works  with relational databases like TeraData,Netteza,PostGres)
  the target RDBMS must be a part of compatable RDBMS(will support JDBC/ODBC).
* If we are communicating with RDBMS from hadoop using sqoop ,the RDBMS  specific connector jar should be a part of 
  $SQOOP_HOME/lib
  mysql-connector-java-5.1.38.jar
* Sqoop uses MapReduces to import & export the data, which provides parallel operation as well as fault tolerance.
* Sqoop command submitted by the end user is parsed by sqoop & launched as hadoop Map only job to import & export data because reduce
  is required only when aggregations are needed & sqoop does not provide any aggregations.
* Sqoop also porvide the ability to import from SQL databases straight into your Hive data warehouse.

Note: SQOOP is only meant for import of data of RDBMS table but not the logic (triggers, cursor or stored procedure) to be applied 
on table data.
