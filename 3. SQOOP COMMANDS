 -----------TO CHECK MYSQL “DATABASES” , “TABLES”  & “TABLES DATA” 

sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root --password root;
sqoop list-tables --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root;

sqoop eval --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root --query "select * from emp limit 3"
sqoop eval --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root --query "select * from emp where esal > 25000";

sqoop eval --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt --query "show tables";

realtime scenario:
$ sqoop import --connect jdbc:mysql://database.example.com/employees   
[will connect to a MySQL database named employees on the host database.example.com]

-----------Commands Available in sqoop <sqoop-help>:

  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  import-mainframe   Import datasets from a mainframe server to HDFS
  job                Work with saved jobs
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  merge              Merge results of incremental imports
  metastore          Run a standalone Sqoop metastore
  version            Display version information

-------------------Import control arguments
Argument 	                    Description
-------------------------------------------------------------
--append 	                    Append data to an existing dataset in HDFS
--as-avrodatafile 	           Imports data to Avro Data Files
--as-sequencefile 	           Imports data to SequenceFiles
--as-textfile 	               Imports data as plain text (default)
--boundary-query <statement> 	Boundary query to use for creating splits
--columns <col,col,col…> 	    Columns to import from table
--direct                     	Use direct import fast path
--direct-split-size <n> 	     Split the input stream every n bytes when importing in direct mode
--inline-lob-limit <n> 	      Set the maximum size for an inline LOB
-m,--num-mappers <n> 	        Use n map tasks to import in parallel
-e,--query <statement> 	      Import the results of statement.
--split-by <column-name> 	    Column of the table used to split work units
--table <table-name> 	        Table to read
--target-dir <dir> 	          HDFS destination dir
--warehouse-dir <dir> 	       HDFS parent for table destination
--where <where clause> 	      WHERE clause to use during import
-z,--compress 	               Enable compression
--compression-codec <c> 	     Use Hadoop codec (default gzip)
--null-string <null-string> 	 The string to be written for a null value for string columns
--null-non-string <null-string> 	The string to be written for a null value for non-string columns

The --null-string and --null-non-string arguments are optional.\ If not specified, then the string "null" will be used.


-------DATA INGETION ON HDFS------------------------------------------------------------------------------
-----------TO IMPORT THE DATA FROM “RDBMS” to “HDFS”

sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root –password root --table emp;
sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root  --table emp --target-dir=/Import -m 1;
sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root --table emp --target-dir=/Import1 -m 2;
sqoop import --options-file /home/gopalkrishna/Desktop/SQOOP/connectionfile.txt --table emp -m 1 --target-dir=/sqoop1/empnew;

-----------TO IMPORT ALL TABLES DATA FROM RDBMS TO DEFAULT PATH OF HDFS

sqoop import-all-tables --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt -m 1;

-----------TO IMPORT ALL TABLES DATA FROM RDBMS TO HIVE WAREHOUSE PATH

sqoop import-all-tables --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt -m 1 
--warehouse-dir /user/hive/warehouse/sqoopdata.db;

-----------TO IMPORT SELECTED TABLES OF DATABASE [--exclude-tables]

sqoop import-all-tables --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt -m 1 --exclude-tables depttab,emp,student;

-----------TO IMPORT RANGE OF DATA FROM A PARTICULAR TABLE [--boundary-query]

sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root  --table emp 
--target-dir /ImportBoundaryDirNew  -m 1  --boundary-query "select 100,105 from emp";

-----------IMPORTING THE DATA FROM RDBMS TO HDFS IN DIFFRENT FORMS

sqoop import --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt -m 1 --table emp --as-textfile 
--target-dir /IMPORTTEXTFILE;
sqoop import --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt -m 1 --table emp --as-sequencefile 
--target-dir /IMPORT_SEQFILE;
sqoop import --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt -m 1 --table emp --as-avrodatafile 
--target-dir /IMPORT_AVROFILE;

-----------IMPORT THE DATA WITH SELECTED COLUMNS DATA ONLY

sqoop import --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt -m 1 --table emp --columns 'ename,esal'
--target-dir /ImportCols;

-----------IMPORT THE DATA IN A SPECIFIED FORMAT [other than Default - CSV]

sqoop import --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt --table emp 
--target-dir /ImportFormat --fields-terminated-by '\t' -m 1;

-----------IMPORT THE DATA WITH SELECTED CRITERIA [--where]

sqoop import --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt --table emp 
--target-dir /ImportCond --fields-terminated-by '|' --where 'esal > 22000' -m 1;

-----------IMPORT THE DATA WITH “SPLIT-BY” OPTION

sqoop import --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt --table emp 
--target-dir /ImportSplit --fields-terminated-by '|' --where 'esal > 22000' --split-by empid -m 1;

-----------IMPORTING THE DATA WITH “COMPRESSION TECHNIQUES”

By default, data is not compressed. You can compress your data by using the deflate (gzip) algorithm with the -z or --compress argument,
or specify any Hadoop compression codec using the --compression-codec argument.
This applies to SequenceFile, text, and Avro files.

sqoop import --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt --table emp 
--compression-codec GzipCodec -m 1 --target-dir /ImportGZIP;

sqoop import --options-file /home/gopalkrishna/PRAC/SQOOP/ConnectionDetails.txt --table emp 
--compression-codec BZip2Codec -m 1 --target-dir /ImportBZIP

sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root --table student -z

-----------IMPORT WITH “query” OPTION [\$CONDITIONS]

sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root 
--query "select * from emp WHERE \$CONDITIONS and <conditions>" --target-dir /ImportQuery1 -m 1;

sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root -password root --query "select * from depttab
where \$CONDITIONS and deptid>103" -m 1 --target-dir /user/gopalkrishna/depttab

sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root 
--query "select e.empid,ename,esal,deptid,dname,dloc from emp e JOIN dept d ON (e.empid = d.empid) AND \$CONDITIONS" -m 1 
--target-dir /INNERJOIN26;

sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root --query 
"select e.empid,ename,esal,deptid,dname,dloc from emp e LEFT OUTER JOIN dept d ON (e.empid = d.empid) AND \$CONDITIONS" -m 1 --target-dir=/LEFTJOIN26;
sqoop import --connect jdbc:mysql://localhost:3306/gopaldb --username root --password root --query 
"select e.empid,ename,esal,deptid,dname,dloc from emp e RIGHT OUTER JOIN dept d ON (e.empid = d.empid) AND \$CONDITIONS" -m 1 --target-dir=/RIGHTJOIN26;

